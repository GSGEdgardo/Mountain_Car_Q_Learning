{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "bd7e13f0fefbdfc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:48.998537Z",
     "start_time": "2025-04-27T23:12:48.960519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f4977cd8c96dceb5",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.091240Z",
     "start_time": "2025-04-27T23:12:49.076199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "#env = gym.make('MountainCar-v0', render_mode=\"human\") <- por si se quiere ver el vehículo en movimiento\n",
    "\n",
    "\"\"\"\n",
    "Esta linea muestra la cantidad de acciones que tiene nuestro entorno\n",
    "Para este caso el Action Space son 3, y son:\n",
    "0 -> Acelerar a la izquierda\n",
    "1 -> No hacer nada\n",
    "2 -> Acelerar a la derecha\n",
    "\"\"\"\n",
    "env.action_space"
   ],
   "id": "42f203f681432a4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.168551Z",
     "start_time": "2025-04-27T23:12:49.152905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Esta linea muestra la cantidad de estados que tiene nuestro entorno\n",
    "Para este caso el Observation Space son 2, y son:\n",
    "0 -> Posicion de la montaña en el eje X (entre -1.2 y 0.6)\n",
    "1 -> Velocidad del auto (entre -0.07 y 0.07)\n",
    "\"\"\"\n",
    "env.observation_space\n"
   ],
   "id": "969ea5ee9913feec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.263456Z",
     "start_time": "2025-04-27T23:12:49.248902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def discretizar(valor):\n",
    "    aux = ((valor - env.observation_space.low)/(env.observation_space.high - env.observation_space.low))*20\n",
    "    return tuple(aux.astype(np.int32))"
   ],
   "id": "5508f326fa7b9473",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.510564Z",
     "start_time": "2025-04-27T23:12:49.311399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "env.reset() manda dos valores, el primero es el estado inicial y el segundo es un diccionario con información adicional\n",
    "La idea para poder discretizar el estado es que el primer valor de la tupla que retorna env.reset() es el estado inicial\n",
    "\"\"\"\n",
    "estado,_ = env.reset()\n",
    "discretizar(estado)"
   ],
   "id": "4b68428449242ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.605508Z",
     "start_time": "2025-04-27T23:12:49.598506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Ahora toca hacer la q_table, que es una tabla que contiene el valor de cada acción en cada estado\n",
    "La q_table es una matriz de 20x20x3, donde 20 son los estados discretizados y 3 son las acciones posibles\n",
    "Para partir se va a inicializar la q_table con valores aleatorios entre -1 y 1\n",
    "\"\"\"\n",
    "q_table = np.random.uniform(low=-1, high=1, size=[20, 20, 3])"
   ],
   "id": "6f21f77b91706ecd",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fórmula de Q-Learning\n",
    "La actualización de los valores Q sigue esta fórmula:\n",
    "\n",
    "\n",
    "# New Q(s, a) = Q(s, a) + α [ R(s, a) + γ max Q(s', a') - Q(s, a) ]"
   ],
   "id": "694bc79af966574"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Donde:\n",
    "\n",
    "- **New Q(s, a)**: Nuevo valor Q para el estado `s` y acción `a`.\n",
    "- **Q(s, a)**: Valor Q actual para el estado `s` y acción `a`.\n",
    "- **α (alpha)**: Tasa de aprendizaje (Learning Rate).\n",
    "- **R(s, a)**: Recompensa por tomar la acción `a` en el estado `s`.\n",
    "- **γ (gamma)**: Tasa de descuento (Discount Rate), que mide la importancia de las recompensas futuras.\n",
    "- **max Q(s', a')**: Máximo valor esperado de recompensa futura para el siguiente estado `s'` al tomar la mejor acción `a'`.\n",
    "\n",
    "## Resumen visual:\n",
    "\n",
    "| Elemento | Significado |\n",
    "|:---|:---|\n",
    "| **New Q(s, a)** | Nuevo valor Q para el estado y acción |\n",
    "| **Q(s, a)** | Valores actuales de Q |\n",
    "| **α** | Tasa de aprendizaje |\n",
    "| **R(s, a)** | Recompensa obtenida por la acción en el estado |\n",
    "| **γ** | Tasa de descuento |\n",
    "| **max Q(s', a')** | Máxima recompensa futura esperada |\n"
   ],
   "id": "57be00338f631536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:12:49.663313Z",
     "start_time": "2025-04-27T23:12:49.658319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graficar_q_table(q_table):\n",
    "    mapa_acciones = np.argmax(q_table, axis=2)  # Elegimos la mejor acción para cada celda\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(mapa_acciones, cmap='viridis', origin='lower')\n",
    "    plt.colorbar(ticks=[0,1,2], label='Acción: 0=Izquierda, 1=Nada, 2=Derecha')\n",
    "    plt.title('Mapa de acciones preferidas')\n",
    "    plt.xlabel('Velocidad Discretizada')\n",
    "    plt.ylabel('Posición Discretizada')\n",
    "    plt.show()"
   ],
   "id": "b5b1facb7eaba9a2",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:15:05.232885Z",
     "start_time": "2025-04-27T23:12:49.706320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# env.reset(): Esto es para poder cambiar la posición de inicio\n",
    "# alpha: tasa_aprendizaje = 0.1\n",
    "# gamma:  tasa de descuento = 0.95 , si está próximo a cero busca las recompensa más cercanas, si está próximo a uno busca las recompensas más lejanas -> confianza\n",
    "# epsilon: tasa de exploración = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.7\n",
    "iteraciones = 5000\n",
    "lista_recompensas = []\n",
    "\n",
    "# Inicializar grafico de la Q-table\n",
    "plt.ion()  # Modo interactivo\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "img = ax.imshow(np.argmax(q_table, axis=2), cmap='viridis', origin='lower')\n",
    "cbar = plt.colorbar(img, ax=ax, ticks=[0, 1, 2])\n",
    "cbar.ax.set_yticklabels(['Izquierda', 'Nada', 'Derecha'])\n",
    "ax.set_title('Mapa de acciones preferidas (entrenando)')\n",
    "ax.set_xlabel('Velocidad Discretizada')\n",
    "ax.set_ylabel('Posición Discretizada')\n",
    "\n",
    "for iteracion in range(iteraciones):\n",
    "    estado,_ = env.reset()\n",
    "    estado = discretizar(estado)\n",
    "    final = False\n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "        \"\"\"\n",
    "        # 20% que sea aleatoria y 80% que sea la mejor acción\n",
    "        if randint(0, 10) > 2:\n",
    "            accion = np.argmax(q_table[estado])\n",
    "        else:\n",
    "            accion = randint(0, 2)\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            accion = np.random.randint(0,3) # Exploracion -> accion aleatoria\n",
    "        else:\n",
    "            accion = np.argmax(q_table[estado]) # Mejor accion\n",
    "        nuevo_estado, recompensa, final, truncado, info = env.step(accion)\n",
    "        # Esta es la fórmula de Q-Learning, que representa\n",
    "        q_table[estado][accion] = q_table[estado][accion] + alpha * (recompensa + gamma * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "        estado = discretizar(nuevo_estado)\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "\n",
    "        if (iteracion+1) % 1000 == 0:\n",
    "            env.render()\n",
    "\n",
    "    lista_recompensas.append(recompensa_total)\n",
    "    # Decaimiento de epsilon\n",
    "    epsilon = max(0.01, epsilon * 0.995)\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if (iteracion+1) % 100 == 0:\n",
    "        print(f\"Iteración: {iteracion+1}, Recompensa total: {recompensa_total}\")\n",
    "        print(f\"Media de las recompensas: {np.mean(lista_recompensas[-100:])}\")\n",
    "\n",
    "        img.set_data(np.argmax(q_table, axis=2))\n",
    "        ax.set_title(f'Mapa de acciones preferidas (Iteración {iteracion+1})')\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "env.close()\n",
    "# env.close() cierra la ventana de renderizado\n",
    "\n",
    "plt.ioff() # Desactiva el modo interactivo\n",
    "plt.show()"
   ],
   "id": "bc6bd89e174f39db",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 39\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m     accion \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(q_table[estado]) \u001B[38;5;66;03m# Mejor accion\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m nuevo_estado, recompensa, final, truncado, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maccion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Esta es la fórmula de Q-Learning, que representa\u001B[39;00m\n\u001B[0;32m     41\u001B[0m q_table[estado][accion] \u001B[38;5;241m=\u001B[39m q_table[estado][accion] \u001B[38;5;241m+\u001B[39m alpha \u001B[38;5;241m*\u001B[39m (recompensa \u001B[38;5;241m+\u001B[39m gamma \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(q_table[discretizar(nuevo_estado)]) \u001B[38;5;241m-\u001B[39m q_table[estado][accion])\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\mountaincar-env\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\mountaincar-env\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\mountaincar-env\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\mountaincar-env\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py:147\u001B[0m, in \u001B[0;36mMountainCarEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m (position, velocity)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\mountaincar-env\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py:264\u001B[0m, in \u001B[0;36mMountainCarEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    263\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    265\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ventana = 100  # Tamaño de la ventana del promedio\n",
    "promedio_movil = np.convolve(lista_recompensas, np.ones(ventana)/ventana, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(promedio_movil, label=f'Promedio móvil (ventana={ventana})', color='royalblue', linewidth=2)\n",
    "plt.title('Promedio Móvil de Recompensas', fontsize=16)\n",
    "plt.xlabel('Episodios', fontsize=14)\n",
    "plt.ylabel('Recompensa Promedio', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "19da40953836728c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:15:05.375621300Z",
     "start_time": "2025-04-27T22:49:30.197607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(np.convolve(lista_recompensas, np.ones(100) / 100, mode='valid'))\n",
    "plt.title('Promedio móvil de recompensas (ventana=100)')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa promedio')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "5fe638f1abab39c7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T23:15:05.416198600Z",
     "start_time": "2025-04-27T22:49:30.368230Z"
    }
   },
   "cell_type": "code",
   "source": "graficar_q_table(q_table)",
   "id": "dc7c47a07c9144e2",
   "outputs": [],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
