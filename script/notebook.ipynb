{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "bd7e13f0fefbdfc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.307323Z",
     "start_time": "2025-04-27T22:48:19.199453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f4977cd8c96dceb5",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.555256Z",
     "start_time": "2025-04-27T22:48:19.542235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "\n",
    "\"\"\"\n",
    "Esta linea muestra la cantidad de acciones que tiene nuestro entorno\n",
    "Para este caso el Action Space son 3, y son:\n",
    "0 -> Acelerar a la izquierda\n",
    "1 -> No hacer nada\n",
    "2 -> Acelerar a la derecha\n",
    "\"\"\"\n",
    "env.action_space"
   ],
   "id": "42f203f681432a4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.648261Z",
     "start_time": "2025-04-27T22:48:19.633451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Esta linea muestra la cantidad de estados que tiene nuestro entorno\n",
    "Para este caso el Observation Space son 2, y son:\n",
    "0 -> Posicion de la montaña en el eje X (entre -1.2 y 0.6)\n",
    "1 -> Velocidad del auto (entre -0.07 y 0.07)\n",
    "\"\"\"\n",
    "env.observation_space\n"
   ],
   "id": "969ea5ee9913feec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.712333Z",
     "start_time": "2025-04-27T22:48:19.697372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def discretizar(valor):\n",
    "    aux = ((valor - env.observation_space.low)/(env.observation_space.high - env.observation_space.low))*20\n",
    "    return tuple(aux.astype(np.int32))"
   ],
   "id": "5508f326fa7b9473",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.790463Z",
     "start_time": "2025-04-27T22:48:19.774419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "env.reset() manda dos valores, el primero es el estado inicial y el segundo es un diccionario con información adicional\n",
    "La idea para poder discretizar el estado es que el primer valor de la tupla que retorna env.reset() es el estado inicial\n",
    "\"\"\"\n",
    "estado,_ = env.reset()\n",
    "discretizar(estado)"
   ],
   "id": "4b68428449242ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.900246Z",
     "start_time": "2025-04-27T22:48:19.885200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Ahora toca hacer la q_table, que es una tabla que contiene el valor de cada acción en cada estado\n",
    "La q_table es una matriz de 20x20x3, donde 20 son los estados discretizados y 3 son las acciones posibles\n",
    "Para partir se va a inicializar la q_table con valores aleatorios entre -1 y 1\n",
    "\"\"\"\n",
    "q_table = np.random.uniform(low=-1, high=1, size=[20, 20, 3])"
   ],
   "id": "6f21f77b91706ecd",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fórmula de Q-Learning\n",
    "La actualización de los valores Q sigue esta fórmula:\n",
    "\n",
    "\n",
    "# New Q(s, a) = Q(s, a) + α [ R(s, a) + γ max Q(s', a') - Q(s, a) ]"
   ],
   "id": "694bc79af966574"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Donde:\n",
    "\n",
    "- **New Q(s, a)**: Nuevo valor Q para el estado `s` y acción `a`.\n",
    "- **Q(s, a)**: Valor Q actual para el estado `s` y acción `a`.\n",
    "- **α (alpha)**: Tasa de aprendizaje (Learning Rate).\n",
    "- **R(s, a)**: Recompensa por tomar la acción `a` en el estado `s`.\n",
    "- **γ (gamma)**: Tasa de descuento (Discount Rate), que mide la importancia de las recompensas futuras.\n",
    "- **max Q(s', a')**: Máximo valor esperado de recompensa futura para el siguiente estado `s'` al tomar la mejor acción `a'`.\n",
    "\n",
    "## Resumen visual:\n",
    "\n",
    "| Elemento | Significado |\n",
    "|:---|:---|\n",
    "| **New Q(s, a)** | Nuevo valor Q para el estado y acción |\n",
    "| **Q(s, a)** | Valores actuales de Q |\n",
    "| **α** | Tasa de aprendizaje |\n",
    "| **R(s, a)** | Recompensa obtenida por la acción en el estado |\n",
    "| **γ** | Tasa de descuento |\n",
    "| **max Q(s', a')** | Máxima recompensa futura esperada |\n"
   ],
   "id": "57be00338f631536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:48:19.993575Z",
     "start_time": "2025-04-27T22:48:19.979121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graficar_q_table(q_table):\n",
    "    mapa_acciones = np.argmax(q_table, axis=2)  # Elegimos la mejor acción para cada celda\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(mapa_acciones, cmap='viridis', origin='lower')\n",
    "    plt.colorbar(ticks=[0,1,2], label='Acción: 0=Izquierda, 1=Nada, 2=Derecha')\n",
    "    plt.title('Mapa de acciones preferidas')\n",
    "    plt.xlabel('Velocidad Discretizada')\n",
    "    plt.ylabel('Posición Discretizada')\n",
    "    plt.show()"
   ],
   "id": "b5b1facb7eaba9a2",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:52:39.390443Z",
     "start_time": "2025-04-27T22:51:43.105626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# env.reset(): Esto es para poder cambiar la posición de inicio\n",
    "# alpha: tasa_aprendizaje = 0.1\n",
    "# gamma:  tasa de descuento = 0.95 , si está próximo a cero busca las recompensa más cercanas, si está próximo a uno busca las recompensas más lejanas -> confianza\n",
    "# epsilon: tasa de exploración = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.7\n",
    "iteraciones = 5000\n",
    "lista_recompensas = []\n",
    "\n",
    "# Inicializar grafico de la Q-table\n",
    "plt.ion()  # Modo interactivo\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "img = ax.imshow(np.argmax(q_table, axis=2), cmap='viridis', origin='lower')\n",
    "cbar = plt.colorbar(img, ax=ax, ticks=[0, 1, 2])\n",
    "cbar.ax.set_yticklabels(['Izquierda', 'Nada', 'Derecha'])\n",
    "ax.set_title('Mapa de acciones preferidas (entrenando)')\n",
    "ax.set_xlabel('Velocidad Discretizada')\n",
    "ax.set_ylabel('Posición Discretizada')\n",
    "\n",
    "for iteracion in range(iteraciones):\n",
    "    estado,_ = env.reset()\n",
    "    estado = discretizar(estado)\n",
    "    final = False\n",
    "    recompensa_total = 0\n",
    "    while not final:\n",
    "        \"\"\"\n",
    "        # 20% que sea aleatoria y 80% que sea la mejor acción\n",
    "        if randint(0, 10) > 2:\n",
    "            accion = np.argmax(q_table[estado])\n",
    "        else:\n",
    "            accion = randint(0, 2)\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            accion = np.random.randint(0,3) # Exploracion -> accion aleatoria\n",
    "        else:\n",
    "            accion = np.argmax(q_table[estado]) # Mejor accion\n",
    "        nuevo_estado, recompensa, final, truncado, info = env.step(accion)\n",
    "        # Esta es la fórmula de Q-Learning, que representa\n",
    "        q_table[estado][accion] = q_table[estado][accion] + alpha * (recompensa + gamma * np.max(q_table[discretizar(nuevo_estado)]) - q_table[estado][accion])\n",
    "        estado = discretizar(nuevo_estado)\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "\n",
    "        if (iteracion+1) % 1000 == 0:\n",
    "            env.render()\n",
    "\n",
    "    lista_recompensas.append(recompensa_total)\n",
    "    # Decaimiento de epsilon\n",
    "    epsilon = max(0.01, epsilon * 0.995)\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if (iteracion+1) % 100 == 0:\n",
    "        print(f\"Iteración: {iteracion+1}, Recompensa total: {recompensa_total}\")\n",
    "        print(f\"Media de las recompensas: {np.mean(lista_recompensas[-100:])}\")\n",
    "\n",
    "        img.set_data(np.argmax(q_table, axis=2))\n",
    "        ax.set_title(f'Mapa de acciones preferidas (Iteración {iteracion+1})')\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "env.close()\n",
    "# env.close() cierra la ventana de renderizado\n",
    "\n",
    "plt.ioff() # Desactiva el modo interactivo\n",
    "plt.show()"
   ],
   "id": "bc6bd89e174f39db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración: 100, Recompensa total: -249.0\n",
      "Media de las recompensas: -366.35\n",
      "Iteración: 200, Recompensa total: -158.0\n",
      "Media de las recompensas: -223.71\n",
      "Iteración: 300, Recompensa total: -137.0\n",
      "Media de las recompensas: -181.25\n",
      "Iteración: 400, Recompensa total: -213.0\n",
      "Media de las recompensas: -165.59\n",
      "Iteración: 500, Recompensa total: -143.0\n",
      "Media de las recompensas: -158.3\n",
      "Iteración: 600, Recompensa total: -222.0\n",
      "Media de las recompensas: -153.82\n",
      "Iteración: 700, Recompensa total: -152.0\n",
      "Media de las recompensas: -162.73\n",
      "Iteración: 800, Recompensa total: -153.0\n",
      "Media de las recompensas: -182.06\n",
      "Iteración: 900, Recompensa total: -151.0\n",
      "Media de las recompensas: -158.6\n",
      "Iteración: 1000, Recompensa total: -170.0\n",
      "Media de las recompensas: -171.69\n",
      "Iteración: 1100, Recompensa total: -153.0\n",
      "Media de las recompensas: -141.05\n",
      "Iteración: 1200, Recompensa total: -159.0\n",
      "Media de las recompensas: -148.45\n",
      "Iteración: 1300, Recompensa total: -157.0\n",
      "Media de las recompensas: -161.36\n",
      "Iteración: 1400, Recompensa total: -157.0\n",
      "Media de las recompensas: -170.41\n",
      "Iteración: 1500, Recompensa total: -162.0\n",
      "Media de las recompensas: -167.53\n",
      "Iteración: 1600, Recompensa total: -145.0\n",
      "Media de las recompensas: -165.5\n",
      "Iteración: 1700, Recompensa total: -173.0\n",
      "Media de las recompensas: -184.47\n",
      "Iteración: 1800, Recompensa total: -176.0\n",
      "Media de las recompensas: -174.17\n",
      "Iteración: 1900, Recompensa total: -263.0\n",
      "Media de las recompensas: -166.92\n",
      "Iteración: 2000, Recompensa total: -162.0\n",
      "Media de las recompensas: -175.26\n",
      "Iteración: 2100, Recompensa total: -151.0\n",
      "Media de las recompensas: -167.41\n",
      "Iteración: 2200, Recompensa total: -111.0\n",
      "Media de las recompensas: -152.3\n",
      "Iteración: 2300, Recompensa total: -156.0\n",
      "Media de las recompensas: -156.31\n",
      "Iteración: 2400, Recompensa total: -329.0\n",
      "Media de las recompensas: -171.84\n",
      "Iteración: 2500, Recompensa total: -165.0\n",
      "Media de las recompensas: -168.92\n",
      "Iteración: 2600, Recompensa total: -168.0\n",
      "Media de las recompensas: -166.07\n",
      "Iteración: 2700, Recompensa total: -148.0\n",
      "Media de las recompensas: -160.38\n",
      "Iteración: 2800, Recompensa total: -187.0\n",
      "Media de las recompensas: -186.48\n",
      "Iteración: 2900, Recompensa total: -163.0\n",
      "Media de las recompensas: -169.44\n",
      "Iteración: 3000, Recompensa total: -178.0\n",
      "Media de las recompensas: -168.67\n",
      "Iteración: 3100, Recompensa total: -164.0\n",
      "Media de las recompensas: -163.71\n",
      "Iteración: 3200, Recompensa total: -147.0\n",
      "Media de las recompensas: -163.67\n",
      "Iteración: 3300, Recompensa total: -156.0\n",
      "Media de las recompensas: -168.55\n",
      "Iteración: 3400, Recompensa total: -166.0\n",
      "Media de las recompensas: -167.0\n",
      "Iteración: 3500, Recompensa total: -153.0\n",
      "Media de las recompensas: -169.57\n",
      "Iteración: 3600, Recompensa total: -147.0\n",
      "Media de las recompensas: -164.95\n",
      "Iteración: 3700, Recompensa total: -145.0\n",
      "Media de las recompensas: -178.01\n",
      "Iteración: 3800, Recompensa total: -199.0\n",
      "Media de las recompensas: -171.66\n",
      "Iteración: 3900, Recompensa total: -166.0\n",
      "Media de las recompensas: -164.12\n",
      "Iteración: 4000, Recompensa total: -125.0\n",
      "Media de las recompensas: -197.35\n",
      "Iteración: 4100, Recompensa total: -212.0\n",
      "Media de las recompensas: -162.78\n",
      "Iteración: 4200, Recompensa total: -206.0\n",
      "Media de las recompensas: -167.2\n",
      "Iteración: 4300, Recompensa total: -223.0\n",
      "Media de las recompensas: -180.35\n",
      "Iteración: 4400, Recompensa total: -155.0\n",
      "Media de las recompensas: -140.79\n",
      "Iteración: 4500, Recompensa total: -187.0\n",
      "Media de las recompensas: -158.09\n",
      "Iteración: 4600, Recompensa total: -153.0\n",
      "Media de las recompensas: -159.92\n",
      "Iteración: 4700, Recompensa total: -146.0\n",
      "Media de las recompensas: -148.75\n",
      "Iteración: 4800, Recompensa total: -140.0\n",
      "Media de las recompensas: -148.19\n",
      "Iteración: 4900, Recompensa total: -142.0\n",
      "Media de las recompensas: -147.15\n",
      "Iteración: 5000, Recompensa total: -144.0\n",
      "Media de las recompensas: -146.83\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ventana = 100  # Tamaño de la ventana del promedio\n",
    "promedio_movil = np.convolve(lista_recompensas, np.ones(ventana)/ventana, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(promedio_movil, label=f'Promedio móvil (ventana={ventana})', color='royalblue', linewidth=2)\n",
    "plt.title('Promedio Móvil de Recompensas', fontsize=16)\n",
    "plt.xlabel('Episodios', fontsize=14)\n",
    "plt.ylabel('Recompensa Promedio', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "19da40953836728c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:49:30.211632Z",
     "start_time": "2025-04-27T22:49:30.197607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(np.convolve(lista_recompensas, np.ones(100) / 100, mode='valid'))\n",
    "plt.title('Promedio móvil de recompensas (ventana=100)')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa promedio')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "5fe638f1abab39c7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T22:49:30.477142Z",
     "start_time": "2025-04-27T22:49:30.368230Z"
    }
   },
   "cell_type": "code",
   "source": "graficar_q_table(q_table)",
   "id": "dc7c47a07c9144e2",
   "outputs": [],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
